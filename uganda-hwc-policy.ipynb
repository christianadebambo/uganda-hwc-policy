{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13221693,"sourceType":"datasetVersion","datasetId":8380695}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ======================================================================\n# 0) Imports & setup\n# ======================================================================\nimport os, re, math, random, json, warnings\nfrom pathlib import Path\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (\n    roc_auc_score, average_precision_score, classification_report,\n    brier_score_loss\n)\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# external models\n!pip -q install tab-transformer-pytorch xgboost\nfrom tab_transformer_pytorch import TabTransformer\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")\n\n# deterministic-ish\ndef set_seed(s=42):\n    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\nset_seed(42)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATADIR = Path(\"/kaggle/input/kasese-hwc\")\nRAW_COMBINED = DATADIR / \"kasese-hwc-data-2021-combined-2021-2022-partly-cleaned.csv\"\n\nOUTDIR = Path(\"/kaggle/working/outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# 1) Load combined CSV & light cleaning\n# ======================================================================\ndef clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df.columns = (\n        df.columns\n          .str.strip()\n          .str.lower()\n          .str.replace(r\"[^a-z0-9]+\", \"_\", regex=True)\n          .str.strip(\"_\")\n    )\n    return df\n\ndef parse_date_any(x):\n    \"\"\"Robust mixed-format parser: day-first dates; also 'March 2021' -> 1st of month.\"\"\"\n    if pd.isna(x): return pd.NaT\n    s = str(x).strip()\n    # try common exact formats (day-first)\n    for fmt in (\"%d/%m/%Y\", \"%d-%m-%Y\", \"%Y-%m-%d\"):\n        try: return datetime.strptime(s, fmt)\n        except Exception: pass\n    # month-name + year\n    for fmt in (\"%B %Y\", \"%b %Y\"):\n        try:\n            d = datetime.strptime(s, fmt)\n            return d.replace(day=1)\n        except Exception: pass\n    # last resort\n    return pd.to_datetime(s, dayfirst=True, errors=\"coerce\")\n\ndef clean_text(s):\n    if pd.isna(s): return np.nan\n    s = re.sub(r\"\\s+\", \" \", str(s)).strip()\n    return s.title()\n\ndef fix_fields(df: pd.DataFrame) -> pd.DataFrame:\n    # the combined file has these uppercase headers:\n    # DATE, CA, DISTRICT, SUBCOUNTY, PARISH, VILLAGE, WILDLIFE, NATURE OF CONFLICT, ACTION TAKEN\n    df = df.rename(columns={\n        \"date\": \"date\",\n        \"ca\": \"ca\",\n        \"district\": \"district\",\n        \"subcounty\": \"subcounty\",\n        \"parish\": \"parish\",\n        \"village\": \"village\",\n        \"wildlife\": \"species\",\n        \"nature_of_conflict\": \"conflict_type\",\n        \"action_taken\": \"response\",\n        # handle if the original mixed-case survived:\n        \"Date\": \"date\",\n        \"CA\": \"ca\",\n        \"DISTRICT\": \"district\",\n        \"SUBCOUNTY\": \"subcounty\",\n        \"PARISH\": \"parish\",\n        \"VILLAGE\": \"village\",\n        \"WILDLIFE\": \"species\",\n        \"NATURE OF CONFLICT\": \"conflict_type\",\n        \"ACTION TAKEN\": \"response\",\n    }).copy()\n    # normalise simple texts\n    for col in [\"district\",\"subcounty\",\"parish\",\"village\",\"ca\",\"species\",\"conflict_type\",\"response\"]:\n        if col in df.columns:\n            df[col] = df[col].apply(clean_text)\n    return df\n\ndef normalise_conflict(raw):\n    if pd.isna(raw): return np.nan\n    s = re.sub(r\"\\s+\", \" \", str(raw)).strip().lower()\n    s = s.replace(\"damages\", \"damage\").replace(\"destruction\", \"damage\")\n    s = s.replace(\"threats\", \"threat\").replace(\"threatening\", \"threat\")\n    s = s.replace(\"human threatening\", \"human threat\")\n    s = s.replace(\"predationof\", \"predation of \").replace(\"injuryof\", \"injury of \")\n    s = s.replace(\"house damage\", \"property damage\")\n    s = s.replace(\"water tank damage\", \"property damage\").replace(\"food stores damage\",\"property damage\")\n\n    if \"human death\" in s or re.search(r\"\\bdeath\\b\", s): return \"Human Death\"\n    if \"human injury\" in s or \"injured\" in s or \"attack\" in s: return \"Human Injury\"\n    if \"human threat\" in s or \"threat\" in s: return \"Human Threat\"\n    if \"predation\" in s or \"livestock\" in s or re.search(r\"\\b(goat|cow|pig|calf)\\b\", s): return \"Livestock Predation\"\n    if \"property damage\" in s or \"gate wall\" in s: return \"Property Damage\"\n    if \"crop raiding\" in s or \"crop damage\" in s: return \"Crop Damage\"\n    if \"assessment\" in s: return \"Assessment/Admin\"\n    return s.title()\n\nraw = pd.read_csv(RAW_COMBINED, dtype=str)  # keep as string first\nraw = clean_cols(raw)\nraw = fix_fields(raw)\nraw[\"date_parsed\"] = raw[\"date\"].apply(parse_date_any)\n\n# keep core columns\nkeep = [\"date_parsed\",\"district\",\"subcounty\",\"parish\",\"village\",\"ca\",\"species\",\"conflict_type\",\"response\"]\ndf = raw[keep].copy()\n\n# calendar features\ndf[\"month\"] = df[\"date_parsed\"].dt.to_period(\"M\").dt.to_timestamp()\ndf[\"dow\"]   = df[\"date_parsed\"].dt.dayofweek\ndf[\"is_weekend\"] = df[\"dow\"].isin([5,6]).astype(int)\n\n# normalised conflict & severity\ndf[\"conflict_std\"] = df[\"conflict_type\"].apply(normalise_conflict)\nseverity_map = {\n    \"Human Death\": 3,\n    \"Human Injury\": 2,\n    \"Livestock Predation\": 1,\n    \"Crop Damage\": 1,\n    \"Property Damage\": 1,\n    \"Human Threat\": 0,\n    \"Assessment/Admin\": 0,\n}\ndf[\"severity_level\"] = df[\"conflict_std\"].map(severity_map).fillna(1).astype(int)\n\n# repeat incident within 30 days per parish\ndf = df.sort_values([\"parish\",\"date_parsed\"])\ndf[\"next_incident\"] = df.groupby(\"parish\")[\"date_parsed\"].shift(-1)\ndf[\"days_to_next\"]  = (df[\"next_incident\"] - df[\"date_parsed\"]).dt.days\ndf[\"repeat_30d\"]    = ((df[\"days_to_next\"].notna()) & (df[\"days_to_next\"] <= 30)).astype(int)\ndf = df.drop(columns=[\"next_incident\"])\n\n# save a clean copy for reproducibility\nCLEAN_PATH = Path(\"/kaggle/working/data\"); CLEAN_PATH.mkdir(parents=True, exist_ok=True)\nCLEAN_FILE = CLEAN_PATH / \"kasese_hwc_clean_2021_2022.csv\"\ndf.to_csv(CLEAN_FILE, index=False)\nprint(\"Clean rows:\", df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# 2) Train/valid/test split by time: 2021 train, 2022 H1 valid, 2022 H2 test\n# ======================================================================\nwork = df.dropna(subset=[\"date_parsed\"]).copy()\nwork[\"year\"] = work[\"date_parsed\"].dt.year\nwork[\"ym\"]   = work[\"date_parsed\"].dt.to_period(\"M\").dt.to_timestamp()\n\ntrain = work[work[\"year\"] == 2021].copy()\nvalid = work[(work[\"year\"] == 2022) & (work[\"ym\"].dt.month <= 6)].copy()\ntest  = work[(work[\"year\"] == 2022) & (work[\"ym\"].dt.month >= 7)].copy()\n\nprint(\"Split sizes:\", len(train), len(valid), len(test))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# 3) Baseline: multinomial LR (+ Platt calibration), metrics on valid/test\n# ======================================================================\ntarget   = \"severity_level\"\ncat_cols = [\"district\",\"subcounty\",\"parish\",\"village\",\"ca\",\"species\",\"conflict_std\",\"response\"]\nnum_cols = [\"dow\",\"is_weekend\"]\n\npre = ColumnTransformer([\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True), cat_cols),\n    (\"num\", \"passthrough\", num_cols),\n])\n\nbase_lr = LogisticRegression(max_iter=500, class_weight=\"balanced\", multi_class=\"auto\")\npipe = Pipeline([(\"pre\", pre), (\"clf\", base_lr)])\n\nX_tr, y_tr = train[cat_cols + num_cols], train[target]\nX_va, y_va = valid[cat_cols + num_cols], valid[target]\nX_te, y_te = test[cat_cols + num_cols],  test[target]\n\npipe.fit(X_tr, y_tr)\n\n# uncalibrated valid\nprobs_va = pipe.predict_proba(X_va)\nauc_va   = roc_auc_score(y_va, probs_va, multi_class=\"ovr\", average=\"macro\")\nap_va    = average_precision_score(pd.get_dummies(y_va), probs_va, average=\"macro\")\n\n# calibrate on valid\ncal = CalibratedClassifierCV(estimator=pipe, method=\"sigmoid\", cv=\"prefit\")\ncal.fit(X_va, y_va)\nprobs_va_cal = cal.predict_proba(X_va)\n\nauc_va_cal = roc_auc_score(y_va, probs_va_cal, multi_class=\"ovr\", average=\"macro\")\nap_va_cal  = average_precision_score(pd.get_dummies(y_va), probs_va_cal, average=\"macro\")\n\n# brier (per-class mean)\nclasses_sorted = sorted(y_va.unique())\nbrier_uncal = np.mean([brier_score_loss((y_va==k).astype(int), probs_va[:,k]) for k in classes_sorted])\nbrier_cal   = np.mean([brier_score_loss((y_va==k).astype(int), probs_va_cal[:,k]) for k in classes_sorted])\n\nprint(f\"[LR] Valid macro AUC {auc_va:.3f}  AP {ap_va:.3f}\")\nprint(f\"[LR] Calibrated Valid macro AUC {auc_va_cal:.3f}  AP {ap_va_cal:.3f}  \"\n      f\"Brier uncal {brier_uncal:.4f} -> cal {brier_cal:.4f}\")\nprint(classification_report(y_va, probs_va_cal.argmax(1), digits=3, zero_division=0))\n\n# test via calibrated LR\nprobs_te_cal = cal.predict_proba(X_te)\nauc_te = roc_auc_score(y_te, probs_te_cal, multi_class=\"ovr\", average=\"macro\")\nap_te  = average_precision_score(pd.get_dummies(y_te), probs_te_cal, average=\"macro\")\nprint(f\"[LR] Test macro AUC {auc_te:.3f}  AP {ap_te:.3f}\")\nprint(classification_report(y_te, probs_te_cal.argmax(1), digits=3, zero_division=0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# 4) TabTransformer single model (balanced loss) + validation metrics\n# ======================================================================\n# numeric month index (months since 2020-01)\nbase_y, base_m = 2020, 1\nwork[\"month_ix\"] = ((work[\"month\"].dt.year - base_y) * 12\n                    + (work[\"month\"].dt.month - base_m)).astype(\"float32\")\nnum_cols_tt = [\"dow\",\"is_weekend\",\"month_ix\"]\n\ntrain_df = work[work[\"year\"] == 2021].copy()\nvalid_df = work[(work[\"year\"] == 2022) & (work[\"ym\"].dt.month <= 6)].copy()\ntest_df  = work[(work[\"year\"] == 2022) & (work[\"ym\"].dt.month >= 7)].copy()\n\n# vocab from train only\nvocab = {c: {v:i for i,v in enumerate(sorted(\n    train_df[c].astype(\"string\").fillna(\"NA\").unique()\n))} for c in cat_cols}\n\ndef enc_cats(frame):\n    return np.stack([\n        frame[c].astype(\"string\").fillna(\"NA\").map(vocab[c]).fillna(0).astype(int).values\n        for c in cat_cols\n    ], axis=1).astype(\"int64\")\n\nmu, sig = train_df[num_cols_tt].mean(), train_df[num_cols_tt].std().replace(0, 1.0)\ndef enc_nums(frame):\n    return ((frame[num_cols_tt].astype(float).fillna(mu) - mu) / sig).values.astype(\"float32\")\ndef enc_y(frame): return frame[target].astype(int).values\n\nXc_tr, Xn_tr, y_tr_tt = enc_cats(train_df), enc_nums(train_df), enc_y(train_df)\nXc_va, Xn_va, y_va_tt = enc_cats(valid_df), enc_nums(valid_df), enc_y(valid_df)\nXc_te, Xn_te, y_te_tt = enc_cats(test_df),  enc_nums(test_df),  enc_y(test_df)\n\nclass Tds(Dataset):\n    def __init__(self, Xc, Xn, y): self.Xc, self.Xn, self.y = Xc, Xn, y\n    def __len__(self): return len(self.y)\n    def __getitem__(self, i):\n        return torch.from_numpy(self.Xc[i]), torch.from_numpy(self.Xn[i]), torch.tensor(self.y[i], dtype=torch.long)\n\nbs = 64\ntrain_dl = DataLoader(Tds(Xc_tr, Xn_tr, y_tr_tt), bs, shuffle=True)\nvalid_dl = DataLoader(Tds(Xc_va, Xn_va, y_va_tt), bs)\ntest_dl  = DataLoader(Tds(Xc_te, Xn_te, y_te_tt), bs)\n\ncat_cardinalities = [len(vocab[c]) for c in cat_cols]\nn_num      = len(num_cols_tt)\nn_classes  = int(max(y_tr_tt.max(), y_va_tt.max(), y_te_tt.max()) + 1)\n\nmodel = TabTransformer(\n    categories=tuple(cat_cardinalities), num_continuous=n_num,\n    dim=64, depth=2, heads=4, attn_dropout=0.1, ff_dropout=0.1,\n    mlp_hidden_mults=(2,1), mlp_act=nn.ReLU(),\n    num_special_tokens=1, continuous_mean_std=None, dim_out=n_classes\n).to(DEVICE)\n\n# class-balanced CE\nclasses = np.arange(n_classes)\nw_arr = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr_tt)\ncriterion = nn.CrossEntropyLoss(weight=torch.tensor(w_arr.astype(\"float32\"), device=DEVICE))\nopt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=3)\n\ndef run_epoch(dl, train=True, return_logits=False):\n    model.train(train)\n    tot, n = 0.0, 0\n    all_probs, all_y, all_logits = [], [], []\n    for Xc, Xn, y in dl:\n        Xc, Xn, y = Xc.to(DEVICE), Xn.to(DEVICE), y.to(DEVICE)\n        if train: opt.zero_grad()\n        logits = model(Xc, Xn)\n        loss = criterion(logits, y)\n        if train:\n            loss.backward(); opt.step()\n        tot += float(loss.item()) * y.size(0); n += y.size(0)\n        all_logits.append(logits.detach().cpu())\n        all_probs.append(torch.softmax(logits,1).detach().cpu().numpy())\n        all_y.append(y.detach().cpu().numpy())\n    probs = np.vstack(all_probs)\n    ys    = np.concatenate(all_y)\n    if return_logits:\n        return tot/n, probs, ys, torch.cat(all_logits, dim=0).numpy()\n    return tot/n, probs, ys\n\nbest, bad, patience = 1e9, 0, 8\nfor epoch in range(50):\n    tr_loss, _, _ = run_epoch(train_dl, True)\n    va_loss, va_probs, va_y = run_epoch(valid_dl, False)\n    sched.step(va_loss)\n    print(f\"[TT] epoch {epoch+1:02d}  train {tr_loss:.4f}  valid {va_loss:.4f}\")\n    if va_loss < best - 1e-4:\n        best, bad = va_loss, 0\n        torch.save(model.state_dict(), \"/kaggle/working/tabtr_best.pth\")\n    else:\n        bad += 1\n        if bad >= patience: break\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/tabtr_best.pth\", map_location=DEVICE))\n\n# valid/test metrics\n_, va_probs, va_y = run_epoch(valid_dl, False)\n_, te_probs, te_y = run_epoch(test_dl, False)\n\nprint(\"[TT] Valid AUC\",\n      roc_auc_score(va_y, va_probs, multi_class=\"ovr\", average=\"macro\"),\n      \"AP\", average_precision_score(pd.get_dummies(pd.Series(va_y)), va_probs, average=\"macro\"))\nprint(classification_report(va_y, va_probs.argmax(1), digits=3, zero_division=0))\n\nprint(\"[TT] Test  AUC\",\n      roc_auc_score(te_y, te_probs, multi_class=\"ovr\", average=\"macro\"),\n      \"AP\", average_precision_score(pd.get_dummies(pd.Series(te_y)), te_probs, average=\"macro\"))\nprint(classification_report(te_y, te_probs.argmax(1), digits=3, zero_division=0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# 5) Small ensemble + temperature scaling on valid + conformal sets\n# ======================================================================\ndef build_model():\n    return TabTransformer(\n        categories=tuple(cat_cardinalities), num_continuous=n_num,\n        dim=64, depth=2, heads=4, attn_dropout=0.1, ff_dropout=0.1,\n        mlp_hidden_mults=(2,1), mlp_act=nn.ReLU(),\n        num_special_tokens=1, continuous_mean_std=None, dim_out=n_classes\n    ).to(DEVICE)\n\ndef train_one(model, Xc_tr, Xn_tr, y_tr, Xc_va, Xn_va, y_va,\n              class_weights, lr=1e-3, wd=1e-4, max_epochs=40, patience=6):\n    w = torch.tensor(class_weights.astype(\"float32\"), device=DEVICE)\n    crit = nn.CrossEntropyLoss(weight=w)\n    opt  = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n    sched= torch.optim.lr_scheduler.ReduceLROnPlateau(opt, \"min\", 0.5, 3)\n\n    def run_np(Xc, Xn, y, train=True, bs=64, return_logits=False):\n        model.train(train)\n        tot, n = 0.0, 0\n        P, Y, L = [], [], []\n        for i in range(0, len(y), bs):\n            xb_c = torch.from_numpy(Xc[i:i+bs]).to(DEVICE)\n            xb_n = torch.from_numpy(Xn[i:i+bs]).to(DEVICE)\n            yb   = torch.from_numpy(y[i:i+bs]).long().to(DEVICE)\n            if train: opt.zero_grad()\n            logits = model(xb_c, xb_n); loss = crit(logits, yb)\n            if train: loss.backward(); opt.step()\n            tot += float(loss.item())*yb.size(0); n += yb.size(0)\n            P.append(torch.softmax(logits,1).detach().cpu().numpy())\n            Y.append(yb.cpu().numpy()); L.append(logits.detach().cpu().numpy())\n        probs = np.vstack(P); yy = np.concatenate(Y); ll = np.vstack(L)\n        if return_logits: return tot/n, probs, yy, ll\n        return tot/n, probs, yy\n\n    best, bad = 1e9, 0\n    for ep in range(max_epochs):\n        tr_loss, _, _ = run_np(Xc_tr, Xn_tr, y_tr, True)\n        va_loss, _, _ = run_np(Xc_va, Xn_va, y_va, False)\n        sched.step(va_loss)\n        print(f\"  [ens] epoch {ep+1:02d}  train {tr_loss:.4f}  valid {va_loss:.4f}\")\n        if va_loss < best - 1e-4:\n            best, bad = va_loss, 0\n            torch.save(model.state_dict(), \"/kaggle/working/tabtr_best_tmp.pth\")\n        else:\n            bad += 1\n            if bad >= patience: break\n    model.load_state_dict(torch.load(\"/kaggle/working/tabtr_best_tmp.pth\", map_location=DEVICE))\n    return model\n\nclasses = np.arange(n_classes)\nclass_weights = compute_class_weight(\"balanced\", classes=classes, y=y_tr_tt)\n\nseeds = [11,22,33,44,55]\nens = []\nfor i, s in enumerate(seeds, 1):\n    print(f\"\\n=== ensemble member {i}/{len(seeds)} (seed {s}) ===\")\n    torch.manual_seed(s); np.random.seed(s); random.seed(s)\n    m = build_model()\n    m = train_one(m, Xc_tr, Xn_tr, y_tr_tt, Xc_va, Xn_va, y_va_tt, class_weights)\n    ens.append(m)\n\n@torch.no_grad()\ndef ensemble_logits(models, Xc, Xn, bs=64):\n    \"\"\"Return stacked logits per model (for temperature scaling).\"\"\"\n    logits_per_model = []\n    for mdl in models:\n        L = []\n        for i in range(0, len(Xc), bs):\n            xb_c = torch.from_numpy(Xc[i:i+bs]).to(DEVICE)\n            xb_n = torch.from_numpy(Xn[i:i+bs]).to(DEVICE)\n            L.append(mdl(xb_c, xb_n).cpu().numpy())\n        logits_per_model.append(np.vstack(L))\n    return np.stack(logits_per_model, axis=0)  # [M, N, C]\n\nlogits_va_M = ensemble_logits(ens, Xc_va, Xn_va)  # [M,N,C]\nlogits_te_M = ensemble_logits(ens, Xc_te, Xn_te)\n\n# average logits, then softmax -> probabilities\ndef softmax_np(z):\n    z = z - z.max(axis=1, keepdims=True)\n    e = np.exp(z)\n    return e / e.sum(axis=1, keepdims=True)\n\nlogits_va_mean = logits_va_M.mean(axis=0)\nlogits_te_mean = logits_te_M.mean(axis=0)\n\n# ----- Temperature scaling on valid (optimize scalar T) -----\ndef fit_temperature(logits, y):\n    T = torch.ones(1, requires_grad=True, device=DEVICE)\n    y_t = torch.from_numpy(y).long().to(DEVICE)\n    logit_t = torch.from_numpy(logits).float().to(DEVICE)\n    optT = torch.optim.LBFGS([T], lr=0.1, max_iter=50)\n\n    def _loss():\n        optT.zero_grad()\n        scaled = logit_t / T.clamp_min(1e-3)\n        loss = nn.CrossEntropyLoss()(scaled, y_t)\n        loss.backward()\n        return loss\n\n    optT.step(_loss)\n    return float(T.detach().cpu().item())\n\nT_star = fit_temperature(logits_va_mean, y_va_tt)\nprint(\"Temperature T* =\", T_star)\n\np_va_mean = softmax_np(logits_va_mean / T_star)\np_te_mean = softmax_np(logits_te_mean / T_star)\n\ndef entropy(p):\n    p = np.clip(p, 1e-12, 1); return -(p*np.log(p)).sum(axis=1)\n\nprint(\"valid mean entropy\", float(entropy(p_va_mean).mean()))\nprint(\"test  mean entropy\", float(entropy(p_te_mean).mean()))\n\n# ----- Split conformal sets on valid, evaluate on test -----\ndef split_conformal_threshold(probs_cal, y_cal, alpha=0.10):\n    s = 1.0 - probs_cal[np.arange(len(y_cal)), y_cal]\n    n = len(s)\n    q = np.quantile(s, np.ceil((n + 1) * (1 - alpha)) / n, method=\"higher\")\n    return float(q)\n\nqhat = split_conformal_threshold(p_va_mean, y_va_tt, alpha=0.10)\nS_te = p_te_mean >= (1.0 - qhat)\nemp_cov = (S_te[np.arange(len(y_te_tt)), y_te_tt]).mean()\nmed_size = float(np.median(S_te.sum(axis=1)))\nprint(f\"Conformal qhat={qhat:.3f}  test coverage={emp_cov:.3f}  median set size={med_size:.1f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# 6) Action normalisation (robust) + uplift (multi-arm T-learner)\n# ======================================================================\n# success label: no repeat within 30 days\ndf_u = df.copy()\ndf_u[\"y_success\"] = 1 - df_u[\"repeat_30d\"].astype(int)\n\n# time split for uplift too\ndf_u[\"year\"] = df_u[\"date_parsed\"].dt.year\ndf_u[\"ym\"]   = df_u[\"date_parsed\"].dt.to_period(\"M\").dt.to_timestamp()\ntrain_u = df_u[df_u[\"year\"] == 2021].copy()\nvalid_u = df_u[(df_u[\"year\"] == 2022) & (df_u[\"ym\"].dt.month <= 6)].copy()\ntest_u  = df_u[(df_u[\"year\"] == 2022) & (df_u[\"ym\"].dt.month >= 7)].copy()\n\ndef norm_text(s):\n    s = re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n    return s\n\nP_SCARE   = [r\"\\bscare ?shoot\", r\"\\bscares? ?shoot\", r\"\\bsscare shoot\", r\"\\bscareshooting\"]\nP_LOCAL   = [r\"local methods\", r\"scaring using local methods\", r\"\\bdrum\", r\"\\bvuvuz\", r\"camp fire\"]\nP_ASSESS  = [r\"assessment\", r\"asses+e?ment\", r\"\\bassessment carried out\\b\", r\"pam assessment\", r\"assessment carried out by\"]\nP_MEDICAL = [r\"taken to .*hospital\", r\"taken to hospital\", r\"taken to bwera hospital\"]\nP_CAPTURE = [r\"\\bcaptured\\b\", r\"\\bcuptured\\b\", r\"\\brescued\\b\", r\"\\btranslocated\\b\"]\nP_SENS    = [r\"sensiti[sz]ation\", r\"meeting with bee keepers\"]\nP_COMP    = [r\"\\bcompensa?tion\\b\", r\"compans?ionate|compansionate|compansetion\"]\nP_POST    = [r\"post ?mortem|postmoterm|body (not found|missed)\"]\n\ndef map_action(x):\n    t = norm_text(x)\n    if any(re.search(p, t) for p in P_SCARE):   return \"scare_shooting\"\n    if any(re.search(p, t) for p in P_LOCAL):   return \"local_scaring\"\n    if any(re.search(p, t) for p in P_ASSESS):  return \"assessment\"\n    if any(re.search(p, t) for p in P_MEDICAL): return \"medical\"\n    if any(re.search(p, t) for p in P_CAPTURE): return \"capture_move\"\n    if any(re.search(p, t) for p in P_SENS):    return \"sensitisation\"\n    if any(re.search(p, t) for p in P_COMP):    return \"compensation\"\n    if any(re.search(p, t) for p in P_POST):    return \"postmortem\"\n    return \"other\"\n\nfor part in (train_u, valid_u, test_u):\n    part[\"treat\"] = part[\"response\"].apply(map_action)\n\n# features for uplift outcome models\ncat_u = [\"district\",\"subcounty\",\"parish\",\"village\",\"ca\",\"species\",\"conflict_std\",\"treat\"]\nnum_u = [\"dow\",\"is_weekend\"]\nfeat_u = cat_u + num_u\n\ndef Xyt(frame):\n    X = frame[feat_u].copy()\n    y = frame[\"y_success\"].astype(int).values\n    t = frame[\"treat\"].astype(\"category\")\n    return X, y, t\n\nX_tr_u, y_tr_u, t_tr_u = Xyt(train_u)\nX_va_u, y_va_u, t_va_u = Xyt(valid_u)\nX_te_u, y_te_u, t_te_u = Xyt(test_u)\n\nX_tr_ready = pd.get_dummies(X_tr_u, columns=cat_u, dummy_na=True)\nX_va_ready = pd.get_dummies(X_va_u, columns=cat_u, dummy_na=True).reindex(columns=X_tr_ready.columns, fill_value=0)\nX_te_ready = pd.get_dummies(X_te_u, columns=cat_u, dummy_na=True).reindex(columns=X_tr_ready.columns, fill_value=0)\n\nt_names = list(t_tr_u.cat.categories); K = len(t_names)\nprint(\"treatments:\", t_names)\n\ndef make_xgb(seed=42):\n    return XGBClassifier(\n        n_estimators=400, max_depth=4, learning_rate=0.05,\n        subsample=0.8, colsample_bytree=0.8,\n        eval_metric=\"logloss\", n_jobs=-1, random_state=seed\n    )\n\nclass ConstantProb:\n    def __init__(self, p): self.p = float(p)\n    def fit(self, X, y, sample_weight=None): return self\n    def predict_proba(self, X):\n        n = X.shape[0]; p1 = np.full(n, self.p, dtype=np.float32); p0 = 1. - p1\n        return np.stack([p0, p1], 1)\n\ndef fit_bin(X, y, seed, w=None):\n    y = np.asarray(y)\n    if np.unique(y).size == 1:\n        return ConstantProb(float(y.mean()))\n    clf = make_xgb(seed=seed)\n    clf.fit(X, y, sample_weight=w)\n    return clf\n\n# plain (unweighted) multi-arm T-learner\narm_models_plain = []\nfor k, arm in enumerate(t_names):\n    is_arm = (t_tr_u.values == arm)\n    mt = fit_bin(X_tr_ready[is_arm].values,  y_tr_u[is_arm],  100+k)\n    mc = fit_bin(X_tr_ready[~is_arm].values, y_tr_u[~is_arm], 200+k)\n    arm_models_plain.append((mt, mc))\n\ndef predict_uplift(arm_models, X_df):\n    X = X_df.values if hasattr(X_df, \"values\") else X_df\n    ups = []\n    for mt, mc in arm_models:\n        p_t = mt.predict_proba(X)[:,1]; p_c = mc.predict_proba(X)[:,1]\n        ups.append(p_t - p_c)\n    return np.vstack(ups).T  # [N, K]\n\nuplift_va = predict_uplift(arm_models_plain, X_va_ready)\nuplift_te = predict_uplift(arm_models_plain, X_te_ready)\n\n# AUUC-above-random (Radcliffe-style)\ndef uplift_curve(y, uplift, treat_indicator):\n    order = np.argsort(-uplift)\n    y_ord = y[order]; t_ord = treat_indicator[order]\n    cum_t = np.cumsum(t_ord); cum_c = np.cumsum(1 - t_ord)\n    rate_t = np.cumsum(y_ord * t_ord) / np.maximum(cum_t, 1)\n    rate_c = np.cumsum(y_ord * (1 - t_ord)) / np.maximum(cum_c, 1)\n    gain = cum_t * (rate_t - rate_c)\n    return gain\n\ndef auuc_above_random(y, uplift, t_ind):\n    g = uplift_curve(y, uplift, t_ind)\n    x = np.linspace(0, 1, len(g)); g_rand = g[-1] * x\n    return float(np.trapz(g - g_rand, x))\n\n# quick report for the most common arm in train\nfocal = pd.Series(t_tr_u.values).value_counts().index[0]\nkf = t_names.index(focal)\nprint(\"Valid AUUC above random (plain) for\", focal, \":\",\n      auuc_above_random(y_va_u, uplift_va[:, kf], (t_va_u.values==focal).astype(int)))\n\n# Parish-level recommended action (plain T-learner)\nbest_idx = uplift_te.argmax(1)\npolicy_plain = test_u[[\"parish\",\"subcounty\",\"species\",\"conflict_std\"]].copy()\npolicy_plain[\"best_action\"] = [t_names[i] for i in best_idx]\npolicy_plain[\"pred_gain_success\"] = uplift_te.max(1)\n\nrec_by_parish = (\n    policy_plain.groupby([\"parish\",\"subcounty\"])\n      .agg(best_action=(\"best_action\", lambda s: s.value_counts().idxmax()),\n           mean_pred_gain=(\"pred_gain_success\",\"mean\"),\n           n_cases=(\"best_action\",\"size\"))\n      .reset_index()\n      .sort_values([\"n_cases\",\"mean_pred_gain\"], ascending=[False, False])\n)\nrec_by_parish.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# 7) Causal sanity: multinomial propensities + IPW T-learner + DR + overlap IPS\n# ======================================================================\n# features NOT including treatment for propensity\nprop_cat = [\"district\",\"subcounty\",\"parish\",\"village\",\"ca\",\"species\",\"conflict_std\"]\nprop_num = [\"dow\",\"is_weekend\"]\nX_prop_tr = pd.get_dummies(train_u[prop_cat + prop_num], columns=prop_cat, dummy_na=True)\nX_prop_va = pd.get_dummies(valid_u[prop_cat + prop_num], columns=prop_cat, dummy_na=True).reindex(columns=X_prop_tr.columns, fill_value=0)\nX_prop_te = pd.get_dummies(test_u [prop_cat + prop_num], columns=prop_cat, dummy_na=True).reindex(columns=X_prop_tr.columns, fill_value=0)\n\nt_tr_codes = train_u[\"treat\"].astype(\"category\").cat.codes.values\nt_va_codes = valid_u[\"treat\"].astype(\"category\").cat.codes.values\nt_te_codes = test_u [\"treat\"].astype(\"category\").cat.codes.values\nt_names = list(train_u[\"treat\"].astype(\"category\").cat.categories)\nK = len(t_names)\n\nprop_clf = LogisticRegression(max_iter=400, multi_class=\"multinomial\", solver=\"lbfgs\")\nprop_clf.fit(X_prop_tr, t_tr_codes)\nprop_tr = np.clip(prop_clf.predict_proba(X_prop_tr), 1e-2, 1.0); prop_tr /= prop_tr.sum(1, keepdims=True)\nprop_va = np.clip(prop_clf.predict_proba(X_prop_va), 1e-2, 1.0); prop_va /= prop_va.sum(1, keepdims=True)\nprop_te = np.clip(prop_clf.predict_proba(X_prop_te), 1e-2, 1.0); prop_te /= prop_te.sum(1, keepdims=True)\n\n# outcome features include one-hot of treat (as fitted earlier)\nX_tr_out = pd.get_dummies(train_u[feat_u], columns=cat_u, dummy_na=True)\nX_va_out = pd.get_dummies(valid_u[feat_u], columns=cat_u, dummy_na=True).reindex(columns=X_tr_out.columns, fill_value=0)\nX_te_out = pd.get_dummies(test_u [feat_u], columns=cat_u, dummy_na=True).reindex(columns=X_tr_out.columns, fill_value=0)\ny_tr = train_u[\"y_success\"].astype(int).values\ny_va = valid_u[\"y_success\"].astype(int).values\ny_te = test_u [\"y_success\"].astype(int).values\n\n# IPW: fit per arm\narm_models_ipw = []\nt_tr_vals = train_u[\"treat\"].astype(\"category\").values\nfor k, arm in enumerate(t_names):\n    is_arm = (t_tr_vals == arm)\n    X_t, y_t = X_tr_out[is_arm].values, y_tr[is_arm]\n    w_t = 1.0 / prop_tr[is_arm, k]\n    X_c, y_c = X_tr_out[~is_arm].values, y_tr[~is_arm]\n    w_c = 1.0 / (1.0 - prop_tr[~is_arm, k])\n    mt = fit_bin(X_t, y_t, 300+k, w=w_t)\n    mc = fit_bin(X_c, y_c, 400+k, w=w_c)\n    arm_models_ipw.append((mt, mc))\n\nuplift_va_ipw = predict_uplift(arm_models_ipw, X_va_out)\nuplift_te_ipw = predict_uplift(arm_models_ipw, X_te_out)\n\n# per-arm AUUC-above-random on valid\nscores = []\nt_va_vals = valid_u[\"treat\"].astype(str).values\nfor k, arm in enumerate(t_names):\n    t_ind = (t_va_vals == arm).astype(int)\n    scores.append((arm, auuc_above_random(y_va, uplift_va_ipw[:,k], t_ind)))\nscores_df = pd.DataFrame(scores, columns=[\"arm\",\"auuc_above_random\"]).sort_values(\"auuc_above_random\", ascending=False)\nprint(scores_df)\n\n# policy under IPW uplift: choose argmax\npi_star_idx = uplift_te_ipw.argmax(axis=1)\n# simple IPS on test\nmatch = (t_te_codes == pi_star_idx).astype(int)\np_logged = prop_te[np.arange(len(prop_te)), t_te_codes]\nips = np.mean(match * y_te / np.clip(p_logged, 1e-3, 1.0))\n\n# model-based value: E[success | choose arm] via outcome model mt\ndef outcome_value(arm_models, X_df, chosen_idx):\n    X = X_df.values\n    vals = []\n    for i in range(len(X)):\n        mt, mc = arm_models[chosen_idx[i]]\n        vals.append(mt.predict_proba(X[i:i+1])[:,1][0])\n    return float(np.mean(vals))\n\nv_hat = outcome_value(arm_models_ipw, X_te_out, pi_star_idx)\n\n# doubly robust estimate\ndef doubly_robust(arm_models, X_df, y, logged_idx, chosen_idx, prop):\n    X = X_df.values; N = len(y); est = np.zeros(N, dtype=float)\n    for i in range(N):\n        mt, mc = arm_models[chosen_idx[i]]\n        mu = mt.predict_proba(X[i:i+1])[:,1][0]\n        if logged_idx[i] == chosen_idx[i]:\n            w = 1.0 / np.clip(prop[i, logged_idx[i]], 1e-3, 1.0)\n            est[i] = mu + w*(y[i]-mu)\n        else:\n            est[i] = mu\n    return float(np.mean(est))\n\ndr = doubly_robust(arm_models_ipw, X_te_out, y_te, t_te_codes, pi_star_idx, prop_te)\n\n# overlap-weighted IPS (downweight extreme propensities)\nov_w = prop_te[np.arange(len(prop_te)), t_te_codes] * (1.0 - prop_te[np.arange(len(prop_te)), t_te_codes])\nips_ov = np.sum(ov_w * match * y_te / np.clip(p_logged, 1e-3, 1.0)) / np.sum(ov_w)\n\nprint(f\"IPS={ips:.3f}  Overlap-IPS={ips_ov:.3f}  Model-value={v_hat:.3f}  DR={dr:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# 8) Interpretability & diagnostics\n# ======================================================================\n# 8.1 Logistic: most influential features per class\nohe = pipe.named_steps[\"pre\"].named_transformers_[\"cat\"]\ncat_names = ohe.get_feature_names_out(cat_cols).tolist()\nfeature_names = cat_names + num_cols\ncoef = pipe.named_steps[\"clf\"].coef_  # [C, D]\nclasses_lr = pipe.named_steps[\"clf\"].classes_\n\nrows = []\nfor ci, c in enumerate(classes_lr):\n    w = coef[ci]\n    idx = np.argsort(np.abs(w))[::-1][:25]\n    for j in idx:\n        rows.append({\"class\": int(c), \"feature\": feature_names[j], \"weight\": float(w[j])})\nlr_top = pd.DataFrame(rows)\nlr_top.to_csv(OUTDIR / \"logistic_top_weights.csv\", index=False)\n\n# 8.2 TabTransformer permutation importance (one ensemble model, for speed)\ndef valid_loss(model, Xc, Xn, y, bs=128):\n    model.eval()\n    tot, n = 0.0, 0\n    with torch.no_grad():\n        for i in range(0, len(y), bs):\n            xb_c = torch.from_numpy(Xc[i:i+bs]).to(DEVICE)\n            xb_n = torch.from_numpy(Xn[i:i+bs]).to(DEVICE)\n            yb   = torch.from_numpy(y[i:i+bs]).long().to(DEVICE)\n            logits = model(xb_c, xb_n)\n            loss = nn.functional.cross_entropy(logits, yb)\n            tot += float(loss.item()) * yb.size(0); n += yb.size(0)\n    return tot/n\n\nm0 = ens[0]\nbase_loss = valid_loss(m0, Xc_va, Xn_va, y_va_tt)\n\ndef perm_importance_cat(model, Xc, Xn, y, col_ix, reps=5):\n    losses = []\n    for _ in range(reps):\n        Xc_perm = Xc.copy()\n        Xc_perm[:, col_ix] = np.random.permutation(Xc_perm[:, col_ix])\n        losses.append(valid_loss(model, Xc_perm, Xn, y))\n    return float(np.mean(losses) - base_loss)\n\ndef perm_importance_num(model, Xc, Xn, y, col_ix, reps=5):\n    losses = []\n    for _ in range(reps):\n        Xn_perm = Xn.copy()\n        Xn_perm[:, col_ix] = np.random.permutation(Xn_perm[:, col_ix])\n        losses.append(valid_loss(model, Xc, Xn_perm, y))\n    return float(np.mean(losses) - base_loss)\n\nimp_rows = []\nfor i, c in enumerate(cat_cols):\n    imp_rows.append({\"feature_group\": c, \"perm_loss_increase\": perm_importance_cat(m0, Xc_va, Xn_va, y_va_tt, i)})\nfor j, c in enumerate(num_cols_tt):\n    imp_rows.append({\"feature_group\": c, \"perm_loss_increase\": perm_importance_num(m0, Xc_va, Xn_va, y_va_tt, j)})\n\ntt_importance = pd.DataFrame(imp_rows).sort_values(\"perm_loss_increase\", ascending=False)\ntt_importance.to_csv(OUTDIR / \"tabtransformer_perm_importance.csv\", index=False)\n\n# 8.3 Reliability diagram (top-class prob vs empirical accuracy) on valid\ndef top_p_and_true(probs, y):\n    top = probs.max(1); pred = probs.argmax(1); correct = (pred == y).astype(int)\n    return top, correct\n\n# LR calibrated top p\np_lr = cal.predict_proba(X_va).max(axis=1)\ny_corr_lr = (cal.predict(X_va) == y_va).astype(int)\n\n# TT ensemble top p (after temperature scaling)\np_tt = p_va_mean.max(axis=1); y_corr_tt = (p_va_mean.argmax(1) == y_va_tt).astype(int)\n\ndef plot_reliability(p, corr, label, ax, n_bins=8):\n    # uniform bins on confidence\n    bins = np.linspace(0, 1, n_bins+1)\n    mids = 0.5*(bins[1:] + bins[:-1])\n    accs, confs = [], []\n    for lo, hi in zip(bins[:-1], bins[1:]):\n        idx = (p >= lo) & (p < hi)\n        if idx.sum() == 0: \n            accs.append(np.nan); confs.append(mids[len(confs)]); \n        else:\n            accs.append(corr[idx].mean()); confs.append(p[idx].mean())\n    ax.plot(confs, accs, marker=\"o\", label=label)\n\nfig, ax = plt.subplots(figsize=(5,4))\nplot_reliability(p_lr, y_corr_lr, \"logistic calibrated\", ax)\nplot_reliability(p_tt, y_corr_tt, \"tabtransformer ensemble\", ax)\nax.plot([0,1],[0,1],\"--\",alpha=0.5)\nax.set_xlabel(\"predicted top-class probability\")\nax.set_ylabel(\"empirical accuracy\")\nax.legend()\nplt.tight_layout()\nplt.savefig(OUTDIR / \"reliability_valid.png\", dpi=160)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# 9) Exports for report\n# ======================================================================\n# Policy table on 2022 H2 (from IPW uplift)\npolicy_df = test_u[[\"parish\",\"subcounty\",\"species\",\"conflict_std\",\"treat\",\"y_success\"]].copy()\npolicy_df[\"chosen_action\"] = [t_names[i] for i in pi_star_idx]\npolicy_df[\"agree_logged\"]  = (policy_df[\"chosen_action\"] == test_u[\"treat\"]).astype(int)\npolicy_df[\"pred_top_p\"]    = p_te_mean.max(axis=1)\npolicy_df[\"entropy\"]       = entropy(p_te_mean)\n\npolicy_df.to_csv(OUTDIR / \"policy_kasese_2022H2.csv\", index=False)\nrec_by_parish.to_csv(OUTDIR / \"uplift_recommendations_by_parish.csv\", index=False)\n\nprint(\"\\nSaved files:\")\nfor f in OUTDIR.iterdir():\n    print(\" -\", f)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}